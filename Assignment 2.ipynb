{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "**Due on Thursday, Febrary 21, 2.00pm**\n",
    "\n",
    "Your submission will be a pdf of your Jupyter notebook which would have already ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.** You are given the following list of English vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = [\"a\", \"o\", \"i\", \"u\", \"e\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the idea of a counter, implement a program that asks the user for a word, and then prints the number of consonants in that word. (For simplicity, we assume that \"y\" always behaves as a consonant, even though it is not true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of consonants in linguist: 5\n"
     ]
    }
   ],
   "source": [
    "word_input = input(\"Enter a word: \")\n",
    "\n",
    "consonants_counter = 0\n",
    "for char in word_input:\n",
    "    if char not in vowels:\n",
    "        consonants_counter += 1\n",
    "\n",
    "print(f\"Number of consonants in {word_input}: {consonants_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.**\n",
    "Implement a program that asks the user for the value of $n$ and for a word, and extracts $n$-grams from that word for any $n$ provided by the user.\n",
    "\n",
    "    word:   banana\n",
    "    n:      2\n",
    "    ngrams: ba, an, na, an, na\n",
    "    \n",
    "    word:   linguist\n",
    "    n:      3\n",
    "    ngrams: lin, ing, ngu, gui, uis, ist\n",
    "    \n",
    "    \n",
    "*Hint 1* If you didn't do it for practice before, start by implementing a code that extracts all bigrams. Then think about how you can generalize it to arbitary $n$.\n",
    "\n",
    "*Hint 2* Be careful with the *edges* (i.e., the last $n$ gram in each word). And what happens if the word is shorter then the $n$-gram? (i.e, the word is \"hi\" and n=3? You still need to list \"hi\"!)\n",
    "\n",
    "**Important** There are multiple default libraries to extract $n$-grams, already available in Python. But for this homework you **must** use the concepts we have studies so far. Any solution involving an external library will be counted as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams: ['lin', 'ing', 'ngu', 'gui', 'uis', 'ist']\n"
     ]
    }
   ],
   "source": [
    "word_input = input(\"Enter a word: \")\n",
    "n_input = int(input(\"Enter a number for n-gram: \"))\n",
    "ngram_list = []\n",
    "\n",
    "for i in range(len(word_input) - n_input + 1):\n",
    "    ngram = word_input[i:i+n_input]\n",
    "    ngram_list.append(ngram)\n",
    "\n",
    "if len(word_input) < n_input:\n",
    "    ngram_list.append(word_input)\n",
    "\n",
    "print(f\"ngrams: {ngram_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.** You are given the following text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It was dark, like the bottom of a well. There was a pattern of skulls and bones around \\\n",
    "the frame, for the sake of appearances; Death could not look himself in the skull in a mirror \\\n",
    "with cherubs and roses around it. The Death of Rats climbed the frame in a scrabble of claws and \\\n",
    "looked at Death expectantly from the top. Quoth fluttered over and pecked briefly at his own \\\n",
    "reflection, on the basis that anything was worth a try. Show me, said Death, show me my thoughts. \\\n",
    "A chessboard appeared, but it was triangular, and so big that only the nearest point could be seen. \\\n",
    "Right on this point was the world - turtle, elephants, the little orbiting sun and all. It was the \\\n",
    "Discworld, which existed only just this side of total improbability and, therefore, in border country. \\\n",
    "In border country the border gets crossed, and sometimes things creep into the universe that have \\\n",
    "rather more on their mind than a better life for their children and a wonderful future in the \\\n",
    "fruit picking and domestic service industries. On every other black or white triangle of the \\\n",
    "chessboard, all the way to infinity, was a small grey shape, rather like an empty hooded robe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also given a string that contains all symbols of English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 1._ Write some code that generates the list `unique_words`, containing all and only the unique lowercase words from `text`.\n",
    "\n",
    "You should see the following output (the order can differ!):\n",
    "    \n",
    "    ['a', 'infinity', 'reflection', 'with', 'like', 'big', 'briefly', 'into', 'children', 'which', 'fruit', 'picking', 'there', 'try', 'little', 'around', 'appearances', 'appeared', 'all', 'crossed', 'basis', 'improbability', 'their', 'discworld', 'black', 'to', 'death', 'future', 'only', 'my', 'robe', 'things', 'for', 'it', 'existed', 'said', 'sake', 'sometimes', 'right', 'way', 'that', 'country', 'chessboard', 'quoth', 'well', 'domestic', 'skull', 'wonderful', 'hooded', 'or', 'empty', 'bottom', 'mirror', 'himself', 'rather', 'over', 'every', 'triangle', 'roses', 'border', 'orbiting', 'was', 'from', 'show', 'be', 'pecked', 'bones', 'just', 'universe', 'me', 'triangular', 'gets', 'worth', 'have', 'climbed', 'service', 'fluttered', 'top', 'but', 'grey', 'claws', 'at', 'rats', 'creep', 'own', 'pattern', 'point', 'white', 'than', 'dark', 'therefore', 'frame', 'this', 'not', 'the', 'could', 'mind', 'turtle', 'scrabble', 'better', 'industries', 'looked', 'an', 'cherubs', 'life', 'anything', 'more', 'small', 'and', 'of', 'his', 'on', 'skulls', 'elephants', 'in', 'thoughts', 'seen', 'nearest', 'expectantly', 'other', 'side', 'shape', 'total', 'so', 'world', 'look', 'sun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dark', 'bottom', 'well', 'pattern', 'skulls', 'bones', 'sake', 'appearances', 'not', 'look', 'himself', 'skull', 'mirror', 'with', 'cherubs', 'roses', 'climbed', 'scrabble', 'claws', 'looked', 'expectantly', 'from', 'top', 'fluttered', 'over', 'pecked', 'briefly', 'his', 'own', 'reflection', 'basis', 'anything', 'worth', 'try', 'said', 'show', 'my', 'thoughts', 'appeared', 'but', 'triangular', 'so', 'big', 'nearest', 'be', 'seen', 'world', 'turtle', 'elephants', 'little', 'orbiting', 'sun', 'which', 'existed', 'just', 'side', 'total', 'improbability', 'therefore', 'gets', 'crossed', 'sometimes', 'things', 'creep', 'into', 'universe', 'have', 'more', 'mind', 'than', 'better', 'life', 'children', 'wonderful', 'future', 'fruit', 'picking', 'domestic', 'service', 'industries', 'every', 'other', 'black', 'or', 'white', 'triangle', 'way', 'to', 'infinity', 'small', 'grey', 'shape', 'an', 'empty', 'hooded', 'robe']\n"
     ]
    }
   ],
   "source": [
    "unique_words = []\n",
    "not_unique_words = []\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "for p in punc:\n",
    "    text = text.replace(p, \"\")\n",
    "\n",
    "for word in text.split(\" \"):\n",
    "    if word not in unique_words and word not in not_unique_words:\n",
    "        for c in word:\n",
    "            if c not in alphabet:\n",
    "                break\n",
    "        else:\n",
    "            unique_words.append(word)\n",
    "    elif word in unique_words:\n",
    "        unique_words.remove(word)\n",
    "        not_unique_words.append(word)\n",
    "\n",
    "if unique_words.index(\"\"):\n",
    "    unique_words.remove(\"\")\n",
    "\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 2._ Write a program which generates the list `bigrams`, collect all attested bigrams in `unique_words`. Ignore words that are shorter than $2$ characters. **Make sure that the list `bigrams` does not contain duplicates**.\n",
    "\n",
    "*Hint. You can use the code to extract bigrams you wrote above. Then, you need to have that code iterate over each word in the `unique_words` list and add a check for duplicates!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['da', 'ar', 'rk', 'bo', 'ot', 'tt', 'to', 'om', 'we', 'el', 'll', 'pa', 'at', 'te', 'er', 'rn', 'sk', 'ku', 'ul', 'ls', 'on', 'ne', 'es', 'sa', 'ak', 'ke', 'ap', 'pp', 'pe', 'ea', 'ra', 'an', 'nc', 'ce', 'no', 'lo', 'oo', 'ok', 'hi', 'im', 'ms', 'se', 'lf', 'mi', 'ir', 'rr', 'ro', 'or', 'wi', 'it', 'th', 'ch', 'he', 'ru', 'ub', 'bs', 'os', 'cl', 'li', 'mb', 'be', 'ed', 'sc', 'cr', 'ab', 'bb', 'bl', 'le', 'la', 'aw', 'ws', 'ex', 'xp', 'ec', 'ct', 'ta', 'nt', 'tl', 'ly', 'fr', 'op', 'fl', 'lu', 'ut', 're', 'ov', 've', 'ck', 'br', 'ri', 'ie', 'ef', 'is', 'ow', 'wn', 'ti', 'io', 'ba', 'as', 'si', 'ny', 'yt', 'in', 'ng', 'wo', 'rt', 'tr', 'ry', 'ai', 'id', 'sh', 'ho', 'my', 'ou', 'ug', 'gh', 'ht', 'ts', 'bu', 'ia', 'gu', 'so', 'bi', 'ig', 'st', 'ee', 'en', 'rl', 'ld', 'tu', 'ur', 'ep', 'ph', 'ha', 'rb', 'su', 'un', 'wh', 'ic', 'xi', 'ju', 'us', 'de', 'al', 'mp', 'pr', 'ob', 'il', 'ty', 'fo', 'ge', 'et', 'ss', 'me', 'gs', 'ni', 'iv', 'rs', 'av', 'mo', 'nd', 'if', 'fe', 'dr', 'rf', 'fu', 'ui', 'pi', 'ki', 'do', 'rv', 'vi', 'du', 'ev', 'ac', 'gl', 'wa', 'ay', 'nf', 'fi', 'sm', 'ma', 'gr', 'ey', 'em', 'pt', 'od']\n"
     ]
    }
   ],
   "source": [
    "bigrams = []\n",
    "for word in unique_words:\n",
    "    if len(word) < 2:\n",
    "        continue\n",
    "\n",
    "    for i in range(len(word) - 2 + 1):\n",
    "        ngram = word[i:i+2]\n",
    "        if ngram not in bigrams:\n",
    "            bigrams.append(ngram)\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 3._ Based on the variable `alphabet`, generate all possible bigrams of English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz', 'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz', 'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'ju', 'jv', 'jw', 'jx', 'jy', 'jz', 'ka', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'ku', 'kv', 'kw', 'kx', 'ky', 'kz', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'lh', 'li', 'lj', 'lk', 'll', 'lm', 'ln', 'lo', 'lp', 'lq', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'lx', 'ly', 'lz', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mh', 'mi', 'mj', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'np', 'nq', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'nx', 'ny', 'nz', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'oq', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'pa', 'pb', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rj', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'rx', 'ry', 'rz', 'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sj', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'sx', 'sy', 'sz', 'ta', 'tb', 'tc', 'td', 'te', 'tf', 'tg', 'th', 'ti', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'tp', 'tq', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'tx', 'ty', 'tz', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uh', 'ui', 'uj', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'uq', 'ur', 'us', 'ut', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'wa', 'wb', 'wc', 'wd', 'we', 'wf', 'wg', 'wh', 'wi', 'wj', 'wk', 'wl', 'wm', 'wn', 'wo', 'wp', 'wq', 'wr', 'ws', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xi', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xp', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yt', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n"
     ]
    }
   ],
   "source": [
    "all_bigrams = []\n",
    "for c1 in alphabet:\n",
    "    for c2 in alphabet:\n",
    "        all_bigrams.append(f\"{c1}{c2}\")\n",
    "print(all_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 4._ Collect all unattested bigrams of English in the list `unattested_bigrams`. \n",
    "\n",
    "*Hint* The unattested bigrams are those bigrams that are possible but not attested in the word sample (you collected all attested bigrams before)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be surprised that some bigrams from `unattested_bigrams` are actually present in other English words, the text that we are working with is very small! If you are curious, take a larger text, and run your code on it. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ad', 'ae', 'af', 'ag', 'ah', 'aj', 'am', 'ao', 'aq', 'au', 'ax', 'az', 'bc', 'bd', 'bf', 'bg', 'bh', 'bj', 'bk', 'bm', 'bn', 'bp', 'bq', 'bt', 'bv', 'bw', 'bx', 'by', 'bz', 'ca', 'cb', 'cc', 'cd', 'cf', 'cg', 'ci', 'cj', 'cm', 'cn', 'co', 'cp', 'cq', 'cs', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'db', 'dc', 'dd', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'dp', 'dq', 'ds', 'dt', 'dv', 'dw', 'dx', 'dy', 'dz', 'eb', 'eg', 'eh', 'ei', 'ej', 'ek', 'eo', 'eq', 'eu', 'ew', 'ez', 'fa', 'fb', 'fc', 'fd', 'ff', 'fg', 'fh', 'fj', 'fk', 'fm', 'fn', 'fp', 'fq', 'fs', 'ft', 'fv', 'fw', 'fx', 'fy', 'fz', 'ga', 'gb', 'gc', 'gd', 'gf', 'gg', 'gi', 'gj', 'gk', 'gm', 'gn', 'go', 'gp', 'gq', 'gt', 'gv', 'gw', 'gx', 'gy', 'gz', 'hb', 'hc', 'hd', 'hf', 'hg', 'hh', 'hj', 'hk', 'hl', 'hm', 'hn', 'hp', 'hq', 'hr', 'hs', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'ib', 'ih', 'ii', 'ij', 'ik', 'ip', 'iq', 'iu', 'iw', 'ix', 'iy', 'iz', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'jv', 'jw', 'jx', 'jy', 'jz', 'ka', 'kb', 'kc', 'kd', 'kf', 'kg', 'kh', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'kv', 'kw', 'kx', 'ky', 'kz', 'lb', 'lc', 'lg', 'lh', 'lj', 'lk', 'lm', 'ln', 'lp', 'lq', 'lr', 'lt', 'lv', 'lw', 'lx', 'lz', 'mc', 'md', 'mf', 'mg', 'mh', 'mj', 'mk', 'ml', 'mm', 'mn', 'mq', 'mr', 'mt', 'mu', 'mv', 'mw', 'mx', 'mz', 'na', 'nb', 'nh', 'nj', 'nk', 'nl', 'nm', 'nn', 'np', 'nq', 'nr', 'ns', 'nu', 'nv', 'nw', 'nx', 'nz', 'oa', 'oc', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ol', 'oq', 'ox', 'oy', 'oz', 'pb', 'pc', 'pd', 'pf', 'pg', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pq', 'ps', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'rc', 'rd', 'rg', 'rh', 'rj', 'rm', 'rp', 'rq', 'rw', 'rx', 'rz', 'sb', 'sd', 'sf', 'sg', 'sj', 'sl', 'sn', 'sp', 'sq', 'sr', 'sv', 'sw', 'sx', 'sy', 'sz', 'tb', 'tc', 'td', 'tf', 'tg', 'tj', 'tk', 'tm', 'tn', 'tp', 'tq', 'tv', 'tw', 'tx', 'tz', 'ua', 'uc', 'ud', 'ue', 'uf', 'uh', 'uj', 'uk', 'um', 'uo', 'up', 'uq', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'va', 'vb', 'vc', 'vd', 'vf', 'vg', 'vh', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'wb', 'wc', 'wd', 'wf', 'wg', 'wj', 'wk', 'wl', 'wm', 'wp', 'wq', 'wr', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n",
      "489\n",
      "676\n",
      "187\n"
     ]
    }
   ],
   "source": [
    "unattested_bigrams = []\n",
    "for bigram in all_bigrams:\n",
    "    if bigram not in bigrams:\n",
    "        unattested_bigrams.append(bigram)\n",
    "\n",
    "print(unattested_bigrams)\n",
    "print(len(unattested_bigrams))\n",
    "print(len(all_bigrams))\n",
    "print(len(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4.** You are given two lists, `cities` and `small_cities`. Insert the elements from `small_cities` in `cities` in such a way so that the list `cities` would contain items in the following order:\n",
    "    \n",
    "    [\"NYC\", \"LA\", \"Stony Brook\", \"Provo\", \"SF\"]\n",
    "    \n",
    "Use any method or way you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYC', 'LA', 'Stony Brook', 'Provo', 'SF']\n"
     ]
    }
   ],
   "source": [
    "cities = [\"NYC\", \"LA\", \"SF\"]\n",
    "small_cities = [\"Stony Brook\", \"Provo\"]\n",
    "\n",
    "# your code here\n",
    "cities = cities[0:2] + small_cities + cities[-1:]\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.** Using the given list `cities`, produce the following output:\n",
    "\n",
    "    NYC NYC\n",
    "    NYC LA\n",
    "    NYC SF\n",
    "    LA NYC\n",
    "    LA LA\n",
    "    LA SF\n",
    "    SF NYC\n",
    "    SF LA\n",
    "    SF SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC NYC\n",
      "NYC LA\n",
      "NYC SF\n",
      "LA NYC\n",
      "LA LA\n",
      "LA SF\n",
      "SF NYC\n",
      "SF LA\n",
      "SF SF\n"
     ]
    }
   ],
   "source": [
    "cities = [\"NYC\", \"LA\", \"SF\"]\n",
    "\n",
    "# your code here\n",
    "for c1 in cities:\n",
    "    for c2 in cities:\n",
    "        print(f\"{c1} {c2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6.** You are given some words from the [Swadesh list](https://en.wikipedia.org/wiki/Swadesh_list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sun\", \"moon\", \"earth\", \"water\", \"food\", \"sky\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you are working with a native speaker of some language other than English. Create a new (empty) list for words of that language, call it `translations`. Then, for every word of the Swadesh list (`words`), ask the user to provide its translation, and save them into the `translations` list. After all the words were translated, print `translations`. (ps. for the sake of the hw, you do not need to speak another language! To test your code you can simply input the English word again! What matters is the mapping!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['日', '月亮', '地', '水', '食', '天']\n"
     ]
    }
   ],
   "source": [
    "translations = []\n",
    "\n",
    "for word in words:\n",
    "    tran = input(f\"Enter the translation for {word}: \")\n",
    "    translations.append(tran)\n",
    "\n",
    "print(translations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
